# Deep Learning

Description: Deep learning
Journal/Conference: Nature
Year: 2015
작성일: 2021년 2월 23일
작성자: 익명
태그: Deep Learning

# 번역

딥 러닝은 여러 처리 계층으로 구성된 계산 모델을 통해 여러 수준의 추상화를 가진 데이터의 표현을 학습할 수 있다. 이러한 방법은 음성 인식, 시각적 물체 인식, 물체 감지 및 약물 발견과 유전체학 같은 많은 다른 영역에서 최첨단 기술을 크게 향상시켰다. 딥 러닝은 역 전파 알고리즘을 사용하여 기계가 이전 계층의 표현에서 각 계층의 표현을 계산하는 데 사용되는 내부 매개 변수를 어떻게 변경해야 하는지를 표시함으로써 대규모 데이터 세트의 복잡한 구조를 발견한다. 심층 컨볼루션 네트는 이미지, 비디오, 음성 및 오디오 처리에 획기적인 발전을 가져온 반면, 반복 네트는 텍스트와 음성 같은 순차적 데이터에 빛을 비추었다.

기계 학습 기술은 웹 검색에서 소셜 네트워크의 콘텐츠 필터링, 전자상거래 웹사이트의 추천에 이르기까지 현대 사회의 많은 측면에 힘을 실어주고 있으며, 카메라와 스마트폰과 같은 소비자 제품에 점점 더 많이 존재한다. 기계 학습 시스템은 이미지에서 물체를 식별하고, 음성을 텍스트로 기록하고, 뉴스 항목, 게시물 또는 제품을 사용자의 관심사와 일치시키고, 관련 검색 결과를 선택하는 데 사용된다. 점점 더 이러한 응용 프로그램은 딥 러닝이라는 기술 클래스를 사용한다. 기존의 기계 학습 기법은 자연 데이터를 원시 형태로 처리하는 데 한계가 있었다. 수십 년 동안 패턴 인식 또는 기계 학습 시스템을 구성하는 것은 원시 데이터(예: 이미지의 픽셀 값)를 종종 분류하는 학습 하위 시스템이 적절한 내부 표현 또는 특징 벡터로 변환하는 특징 추출기를 설계하기 위해 신중한 엔지니어링과 상당한 도메인 전문 지식이 필요했다. 즉, 입력의 패턴을 탐지하거나 분류할 수 있습니다.

표현 학습은 기계에 원시 데이터를 공급하고 탐지 또는 분류에 필요한 표현을 자동으로 검색할 수 있는 일련의 방법이다.

딥 러닝 방법은 여러 수준의 표현을 사용하는 표현 학습 방법이며, 각 모듈이 한 수준(원시 입력부터)에서 표현을 더 높고 약간 더 추상적인 수준으로 변환하는 단순하지만 비선형 모듈을 구성함으로써 얻어진다. 이러한 변환이 충분히 구성되면 매우 복잡한 함수를 학습할 수 있습니다. 분류 작업의 경우, 더 높은 표현 계층은 차별에 중요한 입력 측면을 증폭시키고 관련 없는 변동을 억제한다. 예를 들어, 이미지는 픽셀 값의 배열 형태로 제공되며, 첫 번째 표현 계층의 학습된 특징은 일반적으로 이미지의 특정 방향과 위치에서 가장자리의 존재 또는 부재를 나타낸다. 두 번째 레이어는 일반적으로 에지 위치의 작은 변동에 관계없이 에지의 특정 배열을 감지하여 모티브를 감지합니다. 세 번째 계층은 모티브를 익숙한 객체의 일부에 해당하는 더 큰 조합으로 조합할 수 있으며, 후속 계층은 이러한 부품의 조합으로 객체를 감지한다. 딥 러닝의 핵심 측면은 이러한 기능 계층이 인간 엔지니어에 의해 설계되지 않는다는 것이다. 이러한 계층은 범용 학습 절차를 사용하여 데이터에서 학습된다.

딥 러닝은 수년 동안 인공지능 커뮤니티의 최선의 시도에 저항해 온 문제 해결에 큰 진전을 보이고 있다. 고차원 데이터에서 복잡한 구조를 발견하는 데 매우 능숙한 것으로 밝혀졌으며, 따라서 과학, 비즈니스 및 정부의 많은 분야에 적용할 수 있다. 이미지 인식과 음성 인식에서 기록을 능가하는 것 외에도, 잠재적인 약물 분자의 활동을 예측하고, 뇌 회로를 재구성하는 입자 가속기 데이터를 분석하며, 유전자 발현과 질병에 대한 비코딩 DNA의 돌연변이가 미치는 영향을 예측하는 다른 기계 학습 기술을 능가했다. 아마도 더 놀랍게도, 딥 러닝은 자연어 이해, 특히 주제 분류, 감정 분석, 질문 답변 및 언어 번역의 다양한 작업에 대해 매우 유망한 결과를 낳았다.

우리는 딥 러닝이 수작업으로 엔지니어링을 거의 필요로 하지 않기 때문에 사용 가능한 계산과 데이터의 양이 증가하는 것을 쉽게 이용할 수 있기 때문에 가까운 미래에 더 많은 성공을 거둘 것이라고 생각한다. 현재 심층 신경망을 위해 개발 중인 새로운 학습 알고리듬과 아키텍처는 이 진전을 가속화할 뿐이다.

- 지도학습

기계 학습의 가장 일반적인 형태는 깊든 그렇지 않든 간에 지도 학습이다. 우리가 이미지를 집, 자동차, 사람 또는 애완동물로 분류할 수 있는 시스템을 만들고 싶다고 상상해 보세요. 우리는 먼저 집, 자동차, 사람 및 애완동물의 대용량 데이터 세트를 수집합니다. 각 이미지에는 해당 범주가 표시되어 있습니다. 훈련 중에 기계는 이미지를 보여주고 각 범주에 하나씩 점수 벡터 형태로 출력을 생성한다. 우리는 원하는 카테고리가 모든 카테고리 중에서 가장 높은 점수를 받기를 원하지만, 훈련 전에는 이런 일이 일어날 것 같지 않다. 출력 점수와 원하는 점수 패턴 사이의 오차(또는 거리)를 측정하는 목표 함수를 계산한다. 그런 다음 이 오류를 줄이기 위해 내부 조정 가능한 파라미터를 수정합니다. 종종 가중치라고 불리는 이러한 조정 가능한 파라미터는 기계의 입출력 기능을 정의하는 '노브'로 볼 수 있는 실제 숫자이다. 일반적인 딥 러닝 시스템에서는 이러한 조정 가능한 가중치와 기계를 훈련시키는 수억 개의 라벨링된 예가 있을 수 있다.

무게 벡터를 적절히 조정하기 위해 학습 알고리즘은 다음과 같이 일치합니다. 각 가중치에 대해 가중치가 작은 양만큼 증가하면 오차가 얼마나 증가하거나 감소하는지 나타내는 그레이디언트 벡터입니다. 그런 다음 무게 벡터는 그레이디언트 벡터와 반대 방향으로 조정된다. 모든 훈련 예제에 걸쳐 평균을 낸 목표 함수는 중량 값의 고차원 공간에서 일종의 구릉지대로 볼 수 있다. 음의 기울기 벡터는 이 지형에서 가장 가파른 내리막 방향을 나타내며, 출력 오차가 평균 낮은 최소값에 가깝게 접근합니다.

실제로 대부분의 실무자는 확률적 경사 하강(SGD)이라는 절차를 사용한다. 이것은 몇 가지 예에 대한 입력 벡터를 보여주고, 출력과 오류를 계산하며, 그러한 예에 대한 평균 기울기를 계산하고, 그에 따라 가중치를 조정하는 것으로 구성된다. 이 과정은 목표 기능의 평균이 감소하지 않을 때까지 훈련 세트의 많은 작은 예에 대해 반복된다. 각각의 작은 예제 집합은 모든 예에 대한 평균 기울기의 노이즈가 있는 추정치를 제공하기 때문에 확률론적이라고 한다. 이 간단한 절차는 일반적으로 훨씬 정교한 최적화 기법과 비교할 때 놀랄 만큼 빠르게 좋은 가중치 세트를 찾는다. 교육 후, 시스템의 성능은 테스트 세트라고 하는 다른 일련의 예에서 측정됩니다. 이는 기계의 일반화 능력을 테스트하는 데 도움이 됩니다. 즉, 교육 중 본 적이 없는 새로운 입력에 대해 합리적인 답변을 도출할 수 있는 능력입니다.

현재 기계 학습의 많은 실제 응용 프로그램은 수작업으로 설계된 특징 위에 선형 분류기를 사용한다. 2-클래스 선형 분류기는 형상 벡터 성분의 가중 합계를 계산한다. 가중 합계가 임계값을 초과하면 입력은 특정 범주에 속하는 것으로 분류됩니다. 1960년대 이후 우리는 선형 분류기가 입력 공간을 매우 단순한 영역, 즉 하이퍼플레인에 의해 분리된 반 공간으로만 분할할 수 있다는 것을 알고 있었다. 그러나 이미지 및 음성 인식과 같은 문제는 입출력 기능이 특정 미세한 변화(예: 차이)에 매우 민감하면서 물체의 위치, 방향 또는 조도의 변화, 음조 또는 말투의 변화 등 입력의 관련되지 않은 변화에 둔감해야 한다.e는 흰 늑대와 늑대처럼 생긴 사모예드라고 불리는 흰 개의 품종이다. 픽셀 수준에서 서로 다른 포즈와 다른 환경에서 두 Samoyed의 이미지는 서로 매우 다를 수 있는 반면, 동일한 위치와 유사한 배경에 있는 Samoyed와 늑대의 두 이미지는 서로 매우 유사할 수 있다.

예시

다중 계층 신경망(연결된 점으로 표시됨)은 입력 공간을 왜곡하여 데이터 클래스(빨간색과 파란색 라인에 있는 예)를 선형으로 분리할 수 있다. 입력 공간의 일반 그리드(왼쪽에 표시됨)도 숨겨진 단위에 의해 변환되는 방법에 주목하십시오(중앙 패널에 표시됨). 이것은 2개의 입력 단위, 2개의 숨겨진 단위와 1개의 출력 단위를 가진 예시이지만, 객체 인식이나 자연어  처리에 사용되는 네트워크에는 수만 또는 수십만 개의 단위가 포함되어 있다. C의 허가를 받아 복제한다. Olah(http://colah.github.io/).b, 파생상품의 연쇄 법칙)는 두 가지 작은 효과(xony의 작은 변화, yonz의 변화)가 어떻게 구성되는지 알려준다. 작은 변화 βxin x는 βy/βx(즉, 부분 파생물의 정의)를 곱함으로써 먼저 작은 변화 βyin y로 변환된다. 마찬가지로, ➡y의 변화는 ➡zinz를 변화시킨다. 한 방정식을 다른 방정식으로 대체하면 βx가 어떻게 β/βx와 βz/βx 곱에 의해 βz로 변하는가 하는 파생물의 연쇄 법칙이 생긴다. 또한 x, y, z가 벡터일 때(그리고 파생물은 야코비안 행렬) c, 은닉 계층 두 개와 출력 계층 한 개를 가진 신경망에서 전진 패스를 계산하는 데 사용되는 방정식이며, 각각은 모듈을 구성한다. 각 계층에서 우리는 먼저 각 유닛에 대한 총 입력 z를 계산하는데, 이는 아래 계층에서 유닛의 출력에 대한 가중 합계이다. 그런 다음 비선형 함수 f(.)가 z에 적용되어 장치의 출력을 가져옵니다. 단순성을 위해 편향 용어를 생략했습니다. 신경망에 사용되는 비선형 함수는 최근 몇 년 동안 일반적으로 사용되는 정류 선형 단위(ReLU) f(z) = max(0,z)뿐만 아니라 보다 전통적인 sigmoid(예: hyberbolic 접선, f(z) = (exp(z) - exp(z)/(exp)/z) 및 로지스틱 함수 f(z/z)를 포함한다. 뒷길 각 은닉 계층에서 우리는 각 단위의 출력과 관련하여 오류 파생물을 계산하는데, 이는 위 계층의 단위에 대한 총 입력과 관련된 오류 파생물의 가중 합계이다. 그런 다음 출력과 관련된 오류 파생물을 입력에 f(z)의 구배를 곱하여 오류 파생물로 변환한다. 출력 계층에서, 유닛의 출력과 관련된 오류 파생물은 비용 함수를 차별화함으로써 계산된다. 이것은 yl을 준다.

선형 분류기 또는 원시 픽셀에서 작동하는 다른 '희박' 분류기는 앞의 두 분류기를 같은 범주에 넣는 동안 후자의 두 분류기를 구별할 수 없었다. 그렇기 때문에 얕은 분류기에는 선별성-불변성 딜레마를 해결하는 좋은 특징 추출기가 필요하다. 즉, 차별에 중요한 이미지 측면에는 선택적이지만 동물의 자세와 같은 관련 없는 측면에는 불변하는 표현을 생성한다. 분류기를 더 강력하게 만들기 위해 커널 방법과 마찬가지로 일반적인 비선형 기능을 사용할 수 있지만 가우스 커널과 함께 발생하는 것과 같은 일반적인 기능은 학습자가 훈련에서 멀리 떨어진 일반화를 허용하지 않는다. 기존 옵션은 우수한 기능 추출기를 직접 설계하는 것으로, 상당한 양의 엔지니어링 기술과 도메인 전문 지식이 필요하다. 그러나 범용 학습 절차를 사용하여 좋은 기능을 자동으로 학습할 수 있다면 이 모든 것을 피할 수 있다.

- 내 생각 : 얕은 러닝에는 가우시안 필터등 일일히 해줄게 많아서 자동으로 딥러닝으로 갔다

딥 러닝 아키텍처는 단순 모듈의 다층 스택으로, 대부분 학습 대상이며, 비선형 입출력 매핑을 계산한다. 스택의 각 모듈은 입력 정보를 변환하여 표현의 선택성과 불변성을 모두 증가시킵니다. 깊이가 5~20인 여러 비선형 레이어를 사용하면 시스템은 사모예드와 흰 늑대를 구별하는 미세한 세부 사항에 동시에 민감하고 배경, 포즈, 조명 및 주변 객체와 같은 큰 관련 없는 변형에 둔감한 입력의 극히 복잡한 기능을 구현할 수 있다.

- 오차 역전파

패턴 인식 초기부터, 연구원들의 목적은 손으로 만든 기능을 훈련 가능한 다층 네트워크로 교체하는 것이었지만, 그 단순함에도 불구하고, 1980년대 중반까지는 솔루션이 널리 이해되지 않았다. 밝혀진 대로, 다층 아키텍처는 단순한 확률적 경사 하강에 의해 훈련될 수 있다. 모듈이 입력과 내부 가중치의 비교적 부드러운 기능인 한, 역 전파 절차를 사용하여 경사gradients 경사를 계산할 수 있다. 이것이 이루어질 수 있고 효과가 있다는 생각은 1970년대와 1980년대에 여러 다른 집단에 의해 독립적으로 발견되었습니다. 모듈 다층 스택의 가중치와 관련하여 목적 함수의 기울기를 계산하는 역 전파 절차는 파생 모델에 대한 체인 규칙의 실용적인 적용에 지나지 않는다. 핵심 통찰력은 모듈의 입력에 관한 목표의 파생물(또는 기울기)은 해당 모듈의 출력(또는 후속 모듈의 입력)에 관한 그레이디언트에서 역방향으로 작업함으로써 계산될 수 있다는 것이다(그림 1). 역 전파 방정식은 상단의 출력(네트워크가 예측을 생성하는 곳)에서 하단의 (외부 입력이 공급되는 곳)까지 모든 모듈을 통해 그레이디언트를 전파하는 데 반복적으로 적용될 수 있다. 이러한 구배를 계산한 후에는 각 모듈의 가중치에 대한 구배를 계산하는 것이 간단하다.

딥 러닝의 많은 응용 프로그램은 피드포워드 신경망 아키텍처(그림 1)를 사용하여 고정 크기 입력(예: 이미지)을 고정 크기 출력(예: 여러 범주에 대한 확률)에 매핑하는 방법을 학습한다. 한 계층에서 다음 계층으로 이동하기 위해, 단위 집합은 이전 계층에서 입력의 가중 합계를 계산하고 비선형 함수를 통해 결과를 전달한다. 현재 가장 널리 사용되는 비선형 함수는 정류 선형 단위(ReLU)이며, 이는 단순히 반파장 정류기 f(z) = max(z, 0)이다. 지난 수십 년 동안 신경망은 tanh(z) 또는 1/(1 + exp(-z)와 같이 보다 부드러운 비선형성을 사용했지만, ReLU는 일반적으로 많은 계층을 가진 네트워크에서 훨씬 더 빨리 학습하여 비지도 사전 훈련 없이 심층 감독 네트워크를 훈련할 수 있다. 입력 또는 출력 계층에 없는 단위를 은닉 단위라고 합니다. 숨겨진 레이어는 마지막 레이어에 의해 범주가 선형적으로 분리될 수 있도록 비선형 방식으로 입력을 왜곡하는 것으로 볼 수 있다(그림 1).

1990년대 후반, 신경망과 역 전파는 주로 기계 학습 커뮤니티에 의해 버려졌고 컴퓨터 비전 및 음성 인식 커뮤니티에 의해 무시되었다. 유용한 다단계 기능 추출기를 사전 지식이 거의 없는 학습은 불가능하다고 널리 생각되었다. 특히, 단순 경사 하강은 낮은 국소 최소값, 즉 작은 변화도 평균 오차를 감소시키지 않는 중량 구성에 갇힐 것으로 일반적으로 생각되었다. 실제로, 열악한 로컬 최소화는 대형 네트워크에서 거의 문제가 되지 않는다. 초기 조건에 관계없이 시스템은 거의 항상 유사한 품질의 솔루션에 도달합니다. 최근의 이론적 및 경험적 결과는 국소적 최소화가 일반적으로 심각한 문제가 아님을 강력히 시사한다. 대신, 경사는 경사도가 0인 안장점을 조합하여 많은 수의 안장점으로 채워지고, 표면은 대부분의 차원으로 곡선 처리되고 나머지 차원으로 곡선 처리됩니다. 분석 결과, 하향 곡선이 몇 개만 있는 안장점은 매우 큰 수로 존재하지만, 거의 모두가 목표 함수의 값이 매우 유사하다는 것을 알 수 있는 것으로 보인다. 따라서 알고리즘이 어느 안장점에 고착되는지는 그다지 중요하지 않다.

딥피드 포워드 네트워크에 대한 관심은 2006년경 캐나다 고등연구소(CIFAR)가 모여 부활했다. 연구자들은 레이블링된 데이터를 요구하지 않고 형상 검출기의 계층을 생성할 수 있는 비지도 학습 절차를 도입했다. 형상 검출기의 각 레이어를 학습하는 목적은 아래 레이어에서 형상 검출기(또는 원시 입력)의 활동을 재구성하거나 모델링할 수 있는 것이었다. 이 재구성 목표를 사용하여 점진적으로 더 복잡한 형상 검출기의 여러 레이어를 '사전 훈련'함으로써, 심층 네트워크의 가중치가 합리적인 값으로 초기화될 수 있다. 그런 다음 출력 장치의 최종 레이어가 네트워크 상단에 추가되고 전체 심층 시스템이 표준 역전파를 사용하여 미세 조정될 수 있다. 이는 특히 라벨링된 데이터의 양이 매우 제한적일 때 손으로 쓴 숫자를 인식하거나 보행자를 감지하는 데 매우 효과적이었다.

이 사전 훈련 접근법의 첫 번째 주요 적용은 음성 인식이었고, 그것은 프로그래밍하기에 편리하고 연구자들이 10배 또는 20배 더 빨리 네트워크를 훈련할 수 있도록 하는 빠른 그래픽 처리 장치(GPU)의 출현으로 가능해졌다. 2009년, 이 접근법은 음파에서 추출한 계수의 짧은 시간 창을 창 중앙의 프레임으로 나타낼 수 있는 다양한 음성의 조각에 대한 확률 집합에 매핑하는 데 사용되었다. 그것은 작은 어휘를 사용한 표준 음성 인식 벤치마크에서 기록적인 결과를 달성했고, 큰 어휘 작업에 대한 기록적 결과를 제공하기 위해 빠르게 개발되었다. 2012년까지 2009년의 딥넷 버전은 많은 주요 음성 그룹에 의해 개발되었으며 이미 안드로이드 폰에 배포되고 있었다. 소규모 데이터 세트의 경우, 감독되지 않은 사전 훈련은 과적합을 방지하는 데 도움이 되며, 레이블링된 예제의 수가 적을 때 또는 일부 '소스' 작업에 대한 예는 많지만 일부 '대상' 작업에 대한 예는 거의 없는 전송 설정에서 훨씬 더 나은 일반화로 이어진다. 일단 딥 러닝이 재활성화되면, 사전 교육 단계는 소규모 데이터 세트에만 필요한 것으로 나타났습니다.

그러나, 인접한 계층들 사이의 완전한 연결을 가진 네트워크보다 훨씬 더 쉽게 훈련하고 일반화되는 심층 피드포워드 네트워크 종류가 있었다. 이것은 컨볼루션 신경망(ConvNet)이었다. 그것은 신경망이 호의적이지 않았던 기간 동안 많은 실질적인 성공을 거두었고 최근 컴퓨터 비전 커뮤니티에 의해 널리 채택되었다.

- CNN

ConvNets는 여러 어레이의 형태로 제공되는 데이터를 처리하도록 설계되었으며, 예를 들어 3가지 컬러 채널에서 픽셀 강도를 포함하는 3개의 2D 어레이로 구성된 컬러 이미지이다. 많은 데이터 양식은 언어를 포함한 신호와 시퀀스의 경우 1D, 이미지 또는 오디오 스펙트럼 프로그램의 경우 2D, 비디오 또는 볼륨 이미지의 경우 3D 등 여러 배열의 형태로 되어 있다. ConvNets 뒤에는 로컬 연결, 공유 가중치, 풀링 및 많은 계층의 사용이라는 자연 신호의 속성을 활용하는 네 가지 핵심 아이디어가 있다.

일반적인 ConvNet의 아키텍처(그림 2)는 일련의 단계로 구성된다. 처음 몇 단계는 두 가지 유형의 계층, 즉 컨볼루션 레이어와 풀링 레이어로 구성된다. 컨볼루션 레이어의 단위는 피처 맵에서 구성되며, 각 단위는 필터 뱅크라는 가중치 집합을 통해 이전 레이어의 피처 맵에서 로컬 패치에 연결된다. 이 국소 가중 합계의 결과는 ReLU와 같은 비선형성을 통해 전달된다. 피처 맵의 모든 단위는 동일한 필터 뱅크를 공유한다. 계층의 서로 다른 피처 맵은 서로 다른 필터 뱅크를 사용합니다. 이 아키텍처의 이유는 두 가지입니다. 첫째, 이미지와 같은 배열 데이터에서 값의 로컬 그룹은 종종 높은 상관관계를 가지며 쉽게 감지되는 고유한 로컬 모티브를 형성한다. 둘째, 영상 및 기타 신호의 로컬 통계는 위치에 불변한다. 즉, 모티브가 영상의 한 부분에 나타날 수 있다면 어디에나 나타날 수 있기 때문에 서로 다른 위치에 있는 유닛이 동일한 가중치를 공유하고 배열의 다른 부분에서 동일한 패턴을 감지한다는 개념이다. 수학적으로, 피처 맵에 의해 수행되는 필터링 작업은 이산 컨볼루션이므로 이름은

컨볼루션 레이어의 역할은 이전 레이어에서 형상의 로컬 연결을 감지하는 것이지만, 풀링 레이어의 역할은 의미론적으로 유사한 형상을 하나로 병합하는 것이다. 모티브를 형성하는 형상의 상대적 위치가 다소 다를 수 있기 때문에 각 형상의 위치를 거칠게 그려서 모티브를 신뢰성 있게 검출할 수 있다. 일반적인 풀링 장치는 하나의 피쳐 맵(또는 몇 개의 피쳐 맵)에서 단위의 로컬 패치의 최대값을 계산합니다. 인접 풀링 단위는 둘 이상의 행 또는 열에 의해 이동되는 패치에서 입력을 가져와 표현 차원을 줄이고 작은 이동과 왜곡에 대한 불변성을 생성한다. 2단계 또는 3단계의 컨볼루션, 비선형성 및 풀링이 쌓이고 이어서 더 많은 컨볼루션 및 완전 연결 레이어가 이어진다. ConvNet을 통해 그레이디언트를 역 전파하는 것은 일반 심층 네트워크를 통과하는 것만큼 간단하며, 모든 필터 뱅크의 모든 가중치를 훈련시킬 수 있다.

심층 신경망은 많은 자연 신호가 구성 계층이라는 특성을 활용하는데, 여기서 하위 수준을 구성함으로써 더 높은 수준의 특징을 얻는다. 이미지에서 가장자리의 국부적인 조합은 모티브를 형성하고, 모티브는 부품을 조립하며, 부품은 물체를 형성합니다. 음성에서 전화, 음절, 음절, 단어 및 문장에 이르는 언어와 텍스트에도 유사한 계층이 존재한다. 풀링을 사용하면 이전 계층의 요소가 위치와 모양에 따라 달라지는 경우 표현이 거의 달라질 수 없습니다. ConvNets의 컨볼루션 및 풀링 레이어는 시각적 신경 과학의 단순한 세포와 복잡한 세포의 고전적인 개념에서 직접 영감을 받으며, 전체적인 아키텍처는 LGN–V1–V2–V4–을 연상시킨다.시각 피질 복부 경로 내 IT 계층 구조. ConvNet 모델과 원숭이가 동일한 그림을 보여줄 때, ConvNet의 고급 단위의 활성화는 원숭이의 유추상시피질에 있는 160개의 뉴런의 무작위 집합 분산의 절반을 설명한다. ConvNets는 아키텍처가 다소 유사하지만 역 전파와 같은 엔드 투 엔드 감독 학습 알고리듬을 가지고 있지 않은 neocognitron에 뿌리를 두고 있다. 시간 지연 신경망이라 불리는 원시 1D ConvNet이 전화기와 간단한 단어를 인식하는데 사용되었다. 음성 인식 및 문서 읽기를 위한 시간 지연 신경망부터 1990년대 초로 거슬러 올라가는 수많은 컨볼루션 네트워크가 적용되어 왔다. 문서 읽기 시스템은 언어 제약을 구현한 확률적 모델과 공동으로 훈련된 ConvNet을 사용했다. 1990년대 후반까지 이 시스템은 미국의 모든 수표의 10% 이상을 읽고 있었다. 많은 ConvNet 기반의 광학 문자 인식 및 필기 인식 시스템은 나중에 마이크로소프트에 의해 구현되었다. ConvNets는 또한 얼굴과 손을 포함한 자연 이미지에서 물체를 감지하고 얼굴 인식을 위해 1990년대 초에 실험되었다.

2000년대 초부터 ConvNets는 이미지에서 객체와 영역의 탐지, 분할 및 인식에 큰 성공을 거두고 있다. 이것들은 교통 표지 인식, 특히 코넥토믹스를 위한 생물학적 이미지의 분할, 자연 이미지에서 얼굴, 텍스트, 보행자 및 인체의 검출과 같이 라벨링된 데이터가 비교적 풍부한 작업이었다. 소령. ConvNets의 최근 실질적인 성공은 얼굴 인식이다. 중요한 것은 자율 이동 로봇과 자율주행차를 포함한 기술 분야에서 응용 프로그램을 가질 수 있는 픽셀 수준에서 이미지를 라벨링할 수 있다는 것이다. 모바일ye와 NVIDIA와 같은 회사들은 곧 출시될 자동차 비전 시스템에 이러한 ConvNet 기반 방식을 사용하고 있다. 중요성을 얻는 다른 응용 프로그램에는 자연어 이해와 음성 인식이 포함된다.

이러한 성공에도 불구하고 ConvNets는 2012년 ImageNet 대회까지 주류 컴퓨터 비전 및 기계 학습 커뮤니티에 의해 크게 포기되었다. 1,000개의 서로 다른 클래스를 포함하는 웹의 약 백만 개의 이미지 데이터 세트에 심층 컨볼루션 네트워크를 적용했을 때, 그들은 최고의 경쟁 접근법의 오류율을 거의 절반으로 감소시키면서 눈부신 결과를 달성했다. 이러한 성공은 GPU, ReLUs, 드롭아웃이라는 새로운 정규화 기법 및 기존 기술을 변형하여 더 많은 교육 사례를 생성하는 기법에서 비롯되었다. 이 성공은 컴퓨터 비전에 혁명을 가져왔다; ConvNets는 이제 거의 모든 인식 및 감지 작업에 대한 지배적인 접근 방식이며 일부 작업에서 인간 성능에 접근한다. 이미지 캡션 생성을 위한 ConvNets와 반복 넷 모듈을 결합한 최근의 놀라운 데모

최신 ConvNet 아키텍처에는 ReLU 10~20개의 계층, 수억 개의 가중치, 장치 간 수십억 개의 연결이 있다. 이러한 대규모 네트워크를 훈련하는 데 불과 2년 전에만 몇 주가 걸릴 수 있었던 반면, 하드웨어, 소프트웨어 및 알고리듬 병렬화의 진전은 훈련 시간을 몇 시간으로 줄였다. ConvNet 기반의 비전 시스템의 성능은 구글, 페이스북, 마이크로소프트, IBM, 야후를 포함한 대부분의 주요 기술 회사들을 야기시켰다. Twitter와 Adobe는 물론 연구 개발 프로젝트를 시작하고 ConvNet 기반 이미지 이해 제품 및 서비스를 구축하기 위해 빠르게 증가하는 수의 신생 기업입니다. ConvNets는 칩 또는 현장에서 프로그래밍할 수 있는 게이트 어레이에서 효율적인 하드웨어 구현에 쉽게 적응할 수 있다. 엔비디아, 모빌아이, 인텔, 퀄컴, 삼성 등 다수의 기업이 스마트폰, 카메라, 로봇, 자율주행차 등에서 실시간 비전 애플리케이션을 지원하기 위해 컨브넷 칩을 개발하고 있다.

- 분산표현

딥 러닝 이론은 딥 네트가 분산 표현을 사용하지 않는 고전적인 학습 알고리듬에 비해 두 가지 다른 지수적 이점을 가지고 있음을 보여준다. 이러한 두 가지 이점은 모두 구성의 힘에서 비롯되며 적절한 구성 요소 구조를 갖는 기본 데이터 생성 분포에 의존한다. 첫째, 분산 표현을 학습하면 훈련 중에 볼 수 있는 값을 넘어 학습된 기능의 새로운 조합으로 일반화할 수 있다(예: n개의 이진 기능으로 조합이 가능하다). 둘째, 심층 네트워크에서 표현 계층을 구성하면 또 다른 지수 우위 70(깊이 지수)의 잠재력이 생긴다.

다층 신경망의 숨겨진 계층은 목표 출력을 쉽게 예측할 수 있는 방식으로 네트워크의 입력을 나타내는 방법을 배운다. 이는 다층 신경망을 훈련시켜 이전 단어의 로컬 컨텍스트에서 다음 단어를 순차적으로 예측하는 것으로 잘 입증된다. 문맥의 각 단어는 에 제시된다. N 중 하나 벡터로서의 네트워크, 즉 한 성분의 값은 1이고 나머지 성분은 0이다. 첫 번째 계층에서, 각 단어는 다른 패턴의 활성화, 또는 단어 벡터를 생성한다. 언어 모델에서, 네트워크의 다른 계층들은 입력 단어 벡터를 예측된 다음 단어에 대한 출력 단어 벡터로 변환하는 것을 배우는데, 이것은 어휘의 어떤 단어가 다음 단어로 나타날 확률을 예측하는 데 사용될 수 있다. 네트워크는 각각 많은 활성 구성요소를 포함하는 단어 벡터를 학습하는데, 이는 기호에 대한 분산 표현을 학습하는 맥락에서 처음 나타났듯이 단어의 개별 기능으로 해석될 수 있다. 이러한 의미적 특성은 입력에 명시적으로 존재하지 않았다. 이들은 입력과 출력 기호 사이의 구조적 관계를 여러 '마이크로 규칙'으로 인수하는 좋은 방법으로 학습 절차에 의해 발견되었다. 단어 벡터는 단어 순서가 실제 텍스트의 큰 말뭉치에서 왔고 개별 마이크로 규칙을 신뢰할 수 없을 때 매우 잘 작동하는 것으로 밝혀졌다. 예를 들어 뉴스 기사에서 다음 단어를 예측하도록 훈련했을 때, 화요일과 수요일에 학습된 단어 벡터는 스웨덴과 노르웨이의 단어 벡터처럼 매우 유사하다. 요소(특징)가 상호 배타적이지 않고 많은 구성이 관측 데이터에서 보이는 변동에 해당하기 때문에 이러한 표현을 분산 표현이라고 한다. 이 단어 벡터는 전문가가 미리 결정한 것이 아니라 신경망에 의해 자동으로 발견된 학습된 특징으로 구성된다. 텍스트에서 배운 단어의 벡터 표현은 현재 자연어 응용 분야에서 매우 널리 사용되고 있다.

표현 문제는 논리에서 영감을 받은 것과 신경망에서 영감을 받은 인지 패러다임 사이의 논쟁의 핵심에 있다. 논리적으로 영감을 받은 패러다임에서, 상징의 인스턴스는 다른 기호 인스턴스와 동일하거나 동일하지 않은 유일한 속성이다. 그것은 그것의 사용과 관련이 있는 내부 구조를 가지고 있지 않다. 그리고 기호로 추론하기 위해서는, 그것들은 현명하게 선택된 추론 규칙에서 변수에 속박되어야 한다. 대조적으로, 신경망은 단지 큰 활동 벡터, 큰 가중치 행렬 및 스칼라 비선형성을 사용하여 무난한 상식 추론을 뒷받침하는 빠른 '직관적' 추론 유형을 수행한다.

신경 언어 모델을 도입하기 전에는 언어의 통계 모델링에 대한 표준 접근 방식은 분산 표현을 이용하지 않았다. 그것은 최대 N(N-그램이라고 함) 길이의 짧은 기호 시퀀스의 발생 빈도를 계산하는데 기초했다. 가능한 N-그램의 수는 VN의 순서로 되어 있으며, V는 어휘 크기이므로, 소수의 단어 이상의 컨텍스트를 고려하면 매우 큰 교육 코퍼라가 필요할 것이다. Ngrams는 각 단어를 원자 단위로 취급하기 때문에 의미론적으로 관련된 단어의 순서를 일반화할 수 없는 반면, 신경 언어 모델은 각 단어를 실제 가치 있는 특징의 벡터와 연관시킬 수 있고, 의미론적으로 관련된 단어는 벡터 공간에서 서로 가깝게 끝나기 때문에 일반화할 수 있다.

- RNN

역 전파가 처음 도입되었을 때, 그것의 가장 흥미로운 용도는 반복 신경망(RNN)을 훈련시키는 것이었다. 음성 및 언어와 같은 순차적 입력을 수반하는 작업의 경우 RNN을 사용하는 것이 좋다(그림 5). RNN은 입력 시퀀스를 한 번에 하나씩 처리하여 숨겨진 단위로 시퀀스의 모든 과거 요소의 기록에 대한 정보를 암시적으로 포함하는 '상태 벡터'를 유지한다. 서로 다른 이산 시간 단계에서 숨겨진 장치의 출력을 깊은 다층 네트워크에서 서로 다른 뉴런의 출력인 것처럼 고려할 때(그림 5, 오른쪽), RNN을 훈련시키기 위해 역 전파를 적용하는 방법은 명확해진다. RNN은 매우 강력한 동적 시스템이지만, 이를 훈련시킨 것은 사실이다.

역 전파 그레이디언트는 각 시간 단계에서 증가하거나 감소하기 때문에 문제가 있는 것으로 입증되었으며, 따라서 많은 시간 단계에서 일반적으로 폭발하거나 사라진다.

79,80 아키텍처의 진보와 이들을 훈련시키는 방법 덕분에, RNN은 텍스트의 다음 문자나 시퀀스의 다음 단어를 매우 잘 예측하는 것으로 밝혀졌지만, 더 복잡한 작업에도 사용할 수 있다. 예를 들어, 영어 문장을 한 번에 한 단어씩 읽은 후, 숨겨진 단위의 최종 상태 벡터가 문장으로 표현되는 생각을 잘 나타내도록 영어 '인코더' 네트워크를 훈련시킬 수 있다. 그런 다음 이 사고 벡터는 프랑스어 번역의 첫 번째 단어에 대한 확률 분포를 출력하는 공동으로 훈련된 프랑스 '디코더' 네트워크의 초기 숨겨진 상태로 사용될 수 있다. 이 배포에서 특정 첫 번째 단어가 선택되고 디코더 네트워크에 입력으로 제공된 경우, 전체 중지가 선택될 때까지 변환의 두 번째 단어에 대한 확률 분포를 출력합니다. 전반적으로, 이 과정은 영어 문장에 의존하는 확률 분포에 따라 프랑스어 단어의 시퀀스를 생성한다. 이러한 다소 순진한 기계 번역 수행 방식은 빠르게 최첨단 기술과 경쟁적이 되었고, 이는 문장을 이해하려면 추론 규칙을 사용하여 조작되는 내부 상징 표현과 같은 것이 필요한지에 대한 심각한 의구심을 불러일으킨다. 그것은 일상적 추론이 결론에 대한 신뢰성에 기여하는 많은 동시적 유사성을 수반한다는 견해와 더 호환된다.

프랑스 문장의 의미를 영어 문장으로 번역하는 대신 이미지의 의미를 영어 문장으로 '번역'하는 법을 배울 수 있다(그림 3). 여기서 인코더는 픽셀을 마지막 숨겨진 계층에서 활동 벡터로 변환하는 심층 Con-vNet이다. 디코더는 기계 번역 및 신경 언어 모델링에 사용되는 것과 유사한 RNN이다. 최근 이러한 시스템에 대한 관심이 급증하고 있다(참고문헌 86에 언급된 예 참조). RNN은 한 번 펼쳐지면(그림 5) 모든 계층이 동일한 가중치를 공유하는 매우 심층적인 피드포워드 네트워크로 볼 수 있다. 그들의 주된 목적은 장기적인 의존성을 배우는 것이지만, 이론적이고 경험적인 증거는 매우 오랫동안 정보를 저장하는 것을 배우는 것이 어렵다는 것을 보여준다.

이를 해결하기 위해, 하나의 아이디어는 명시적 메모리로 네트워크를 증강하는 것이다. 이러한 종류의 첫 번째 제안은 특수 숨겨진 단위를 사용하는 LSTM(장기 단기 메모리) 네트워크이며, 이 네트워크의 자연 행동은 입력을 오랫동안 기억하는 것이다. 기억 세포라고 불리는 특별한 단위는 축열조나 게이트 누출 뉴런과 같은 역할을 한다: 그것은 하나의 무게가 있는 다음 단계에서 자신과 연결된다. 그래서 그것은 자신의 실제 값을 복사하고 외부 신호를 축적하지만, 이러한 자기 연결은 언제 클리어할지를 결정하는 것을 배우는 다른 단위에 의해 곱셈으로 게이트된다. 기억의 내용

LSTM 네트워크는 그 후, 특히 각 시간 단계에 대해 여러 개의 레이어가 있을 때 기존의 RNN보다 더 효과적인 것으로 입증되어 음향에서 전사의 문자 시퀀스에 이르는 전체 음성 인식 시스템을 가능하게 했다. LSTM 네트워크 또는 관련 형식의 게이트 장치는 현재 기계 번역에 매우 적합한 인코더 및 디코더 네트워크에도 사용되고 있다.

지난 1년 동안, 여러 저자들은 메모리 모듈로 RNN을 증강하는 다른 제안을 해왔다. 제안에는 RNN이 읽거나 쓰기로 선택할 수 있는 '테이프 같은' 메모리에 의해 네트워크가 증강되는 신경 튜링 머신과, 일반적인 네트워크가 일종의 연상 메모리로 증강되는 메모리 네트워크가 포함된다. 메모리 네트워크는 표준 질문 답변 벤치마크에서 우수한 성능을 보였다. 메모리는 네트워크가 나중에 질문에 답하도록 요청받은 이야기를 기억하기 위해 사용됩니다.

단순한 암기를 넘어 신경 튜링 기계와 메모리 네트워크는 일반적으로 추론과 기호 조작이 필요한 작업에 사용되고 있다. 신경 튜링 기계는 '알고리즘'을 가르칠 수 있다. 무엇보다도, 그들은 입력이 정렬되지 않은 순서로 구성되었을 때 정렬된 기호 목록을 출력하는 법을 배울 수 있다. 이 순서는 각 기호가 목록의 우선순위를 나타내는 실제 값에 수반된다. 메모리 네트워크는 텍스트 어드벤처 게임과 유사한 설정에서 세계의 상태를 추적하도록 훈련될 수 있으며, 이야기를 읽은 후에는 복잡한 추론이 필요한 질문에 대답할 수 있다. 한 테스트 예에서, 네트워크는 15문장의 "반지의 제왕" 버전으로 올바르게 표시됩니다. 프로도는 지금 어디에 있는가?와 같은 질문에 답한다.

자연어 이해는 향후 몇 년 동안 딥 러닝이 큰 영향을 미칠 준비가 되어 있는 또 다른 분야이다. 우리는 RNN을 사용하여 문장이나 전체 문서를 이해하는 시스템이 한 번에 한 부분을 선택적으로 참석하기 위한 전략을 학습할 때 훨씬 더 나아질 것으로 기대한다. 궁극적으로, 인공지능의 주요 발전은 표현 학습과 복잡한 추론을 결합한 시스템을 통해 이루어질 것이다. 오랫동안 음성 및 필기 인식에 딥 러닝과 간단한 추론이 사용되었지만, 큰 벡터에 대한 연산에 의한 기호 표현의 규칙 기반 조작을 대체하기 위한 새로운 패러다임이 필요하다.

표현 학습은 기계에 원시 데이터를 공급하고 탐지 또는 분류에 필요한 표현을 자동으로 검색할 수 있는 일련의 방법이다.

딥 러닝 방법은 여러 수준의 표현을 사용하는 표현 학습 방법이며, 각 모듈이 한 수준(원시 입력부터)에서 표현을 더 높고 약간 더 추상적인 수준으로 변환하는 단순하지만 비선형 모듈을 구성함으로써 얻어진다.

첫 번째 표현 계층의 학습된 특징은 일반적으로 이미지의 특정 방향과 위치에서 가장자리의 존재 또는 부재를 나타낸다. 두 번째 레이어는 일반적으로 에지 위치의 작은 변동에 관계없이 에지의 특정 배열을 감지하여 모티브를 감지합니다. 세 번째 계층은 모티브를 익숙한 객체의 일부에 해당하는 더 큰 조합으로 조합할 수 있으며, 후속 계층은 이러한 부품의 조합으로 객체를 감지한다. 딥 러닝의 핵심 측면은 이러한 기능 계층이 인간 엔지니어에 의해 설계되지 않는다는 것이다

컨볼루션 레이어의 역할은 이전 레이어에서 형상의 로컬 연결을 감지하는 것이지만, 풀링 레이어의 역할은 의미론적으로 유사한 형상을 하나로 병합하는 것이다

# 질문

1. 딥러닝과 머신러닝 차이점
- 데이터 학습 추론에 적절한 피처 엔지니어링
- 머신러닝은 원본 데이터를 피처 엔지니어가 직접 적절한 피처를 만들고, 머신러닝 모델의 결과로 아웃풋을 냈었는데, 딥러닝 이후로 Raw데이터를 딥러닝 모델에 넣어주면 모델이 알아서 feature를 알아내고 아웃풋을 내는 형식
- 머신 러닝에서 피처 셀렉션을 하기위해 그 부분만 딥러닝을 사용하는 경우있음
- 유전자 알고리즘은 최적화 방법

정답을 학습하는 방식이 아닌 최선의 결과를 내는 방식을 학습하는 과정 같다고 생각

무엇을 할지 가르쳐주지 않아도 적응하고 학습한다

더 좋은 염색체가 나타나기 전까지는 적합도가 정체될 수 있다

![Deep%20Learning%20f21ab2c754634a39ab18135e6158a042/Untitled.png](Deep%20Learning%20f21ab2c754634a39ab18135e6158a042/Untitled.png)

1. 데이터의 표현이란
- 컴퓨터의 연산을 할 수 있도록, 데이터를 벡터 공간에서의 원소인 벡터로 나타낸 것
1. 딥러닝에서 활성화 함수로 비선형 함수를 사용하는 이유
- NN이 확률을 나타내는 건데 비선형을 써야 확률이 나옴. 렐루쓰고 마지막에 소프트 맥스 사용
- 활성화함수 : 입력 신호의 총합이 활성화 일으키는지 확인하는 함수
- 역전파 과정에서 가중치를 잘 보존해서 학습을 잘 시키기 위함, 어떠한 층의 한 노드의 가중치를 조금만 수정해도 전체적인 시스템이 변화없게 하기위해
- 선형 함수(직선 1개로 표현가능한 곡선)를 사용하면 선형 함수로는 은닉층을 여러번 추가하더라도 1회 추가한 것과 차이를 줄 수 없습니다.
- 선형함수의 정의상 그렇다
- 그래서 사용한 시그모이드는 기울기 소실 문제 발생(x값이 0이나 1일 때 지점에서의 기울기가 0에 가까움)
- 기울기 소실 문제는 앞단에서 학습이 안됨, 앞단이 데이터의 추상적인 특징을 가지고 학습하기 때문에 처음부터 망하면 뒤도 망함, 그래서 앞이 중요함
- 그렇기 때문에 렐루 사용

0과 음수값에 대해서는 0으로 출력, 0초과 값부터는 렐루 함수는 음수를 입력하면 0을 출력하고, 양수를 입력하면 입력값을 그대로 반환합니다. 렐루 함수는 특정 양수값에 수렴하지 않으므로 깊은 신경망에서 시그모이드 함수보다 훨씬 더 잘 작동합니다.

0보다 큰 값일 경우 1을 반환하는 sigmoid와 다르다. 따라서 내부 hidden layer에는 ReLU를 적용하고, 마지막 output layer에서만 sigmoid 함수를 적용하면 이전에 비해 정확도가 훨씬 올라가게 된다.

- XOR 문제해결하기 위해 다층 퍼셉트론
- 퍼셉트론: 1단계 레이어 shallow. 이진분류에서는 정확히 해결됨
- DL:2단계 레이버 deep MLP.이진말고도 여러문제 해결됨

#백그라운드 요약

- 딥러닝 : 쉘로우 러닝과 대비되는 개념으로 은닉층이 2개 이상
- 기계에 로우 데이터를 공급하고 recognition 또는 classification에 필요한값을 자동으로 학습 및 추론할 수 있는 일련의 방법이다.
- 여러 신경망층을 이동하면서 특징을 학습한다(낮은 계층에서는 추상적인 특징을 높은 계층에서는 구체적인 특징 및 분류)
- 어떤 피처가 선택되는 지 사람 엔지니어가 할수 없다는게 특징

첫 번째 표현 계층의 학습된 특징은 일반적으로 이미지의 특정 방향과 위치에서 가장자리의 존재 또는 부재를 나타낸다. 두 번째 레이어는 일반적으로 에지 위치의 작은 변동에 관계없이 에지의 특정 배열을 감지하여 모티브를 감지합니다. 세 번째 계층은 모티브를 익숙한 객체의 일부에 해당하는 더 큰 조합으로 조합할 수 있으며, 후속 계층은 이러한 부품의 조합으로 객체를 감지한다. 딥 러닝의 핵심 측면은 이러한 기능 계층이 인간 엔지니어에 의해 설계되지 않는다는 것이다

컨볼루션 레이어의 역할은 이전 레이어에서 형상의 로컬 연결을 감지하는 것이지만, 풀링 레이어의 역할은 의미론적으로 유사한 형상을 하나로 병합하는 것이다

- 지도학습

비지도학습과는 반대로 데이터 세트의 정답 클래스를 주고 학습을 시키는 머신 러닝 방법으로서, 학습을 진행하면서 각 노드에서 가중치를 조절하는 그레디언트 벡터를 조정하며 가중치를 수정하는 방식

- 역전파
- 핵심은 데이터가 입력된 한 신경망 점에서의 기울기는 해당 점에서 출력이라는점, 그래프가 역방향으로 진행될 수 있음
- 역전파 방정식은 높은 계층의 출력(네트워크가 예측을 생성하는 곳)에서 낮은 계층(데이터 입력이 공급되는 곳)까지 모든 점을 통해 그레이디언트(손실함수의 기울기)를 전파하는 데 반복적으로 적용될 수 있다. 이러한 그래디언트를 계산한 후에는 각 점의 가중치에 대한 그래디언트를 계산 가능해짐
- 옵티마이저가 틀린정보를 노드에 정보전달하고 w,b 조절하는 것이 학습
- 합성곱 신경망

즉 크게는 컨볼루션 레이어와 풀링 레이어로 구성된다. 컨볼루션 레이어는 필터라는 가중치 집합을 통해 합성곱되어 피처 맵에 연결, 각 합성곱된 결과는 활성화 함수에 전달, 이미지의 부분 형상을 감지하는 것이 목적

풀링은 유사한 형상을 하나로 병합하여 특징은 가지되 크기는 줄이느것이 목적

학습은 필터의 가중치를 역전파를 통해 수정하는 과정 자체이다

학습은 어디인가 컨볼류션 레이어에서 필터내의 모양은 정해주지 않고 값들이 어떤 패턴을 찾는 필터를 학습하는 것

사람이 할 수 있는 거는 필터의 개수와 몇바이몇이라는 사이즈뿐

- 분산표현과 언어처리
- 원 핫 인코딩 표현은 각 단어간 유사성을 표현할 수 없다는 단점이 있었고, - 이를 위한 대안으로 단어의 '의미'를 다차원 공간에 벡터화하는 방법을 찾게되는데, 이러한 표현 방법을 분산 표현(distributed representation)
- 분산 표현(distributed representation) 방법은 기본적으로 분포 가설(distributional hypothesis)이라는 가정 하에 만들어진 표현 방법입니다. 이 가정은 '비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다'라는 가정입니다. 강아지란 단어는 귀엽다, 예쁘다, 애교 등의 단어가 주로 함께 등장하는데 분포 가설에 따라서 저런 내용을 가진 텍스트를 벡터화한다면 저 단어들은 의미적으로 가까운 단어가 됩니다. 분산 표현은 분포 가설을 이용하여 단어들의 셋을 학습하고, 벡터에 단어의 의미를 여러 차원에 분산하여 표현합니다.
- 순환 신경망

순환하는 경로가 있고 이를통해 내부에 은닉 상태를 기억할 수 있는 신경망

알엔엔 계층은 그 계층으로의 입력과 1개 전의 알엔엔 계층으로부터의 출력을 입력받음

히든 노드가 방향을 가진 엣지로 연결 구조를 이루는 인공신경망

음성 문자 등 순차적으로 등장하는 데이터 처리에 적합한 모델

시퀀스 길이에 상관없이 인풋과 아웃풋을 수용하는 구조

단점은 그래디언트 문제 : 기울기 감소나 폭발 발생

그래서 중요한 정보를 기억하거나 불필요한 정보를 잊는 기능을 강화한 RNN 모델이 LSTM

https://www.youtube.com/watch?v=bnmAsFBl4E4&list=PLxpDtLP2JQQrPVXZT9fGDX58Tqt731VYl&index=4

T아카데미 cnn 강의 https://www.youtube.com/watch?v=_6GwLjW9sbc